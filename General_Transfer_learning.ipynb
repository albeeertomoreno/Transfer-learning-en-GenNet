{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning adaptado para cualquier capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Rutas a los archivos\n",
    "topology_nuevo_path = '/ceph01/projects/AGRamirez_misc/carpeta_alberto_moreno/datos/transfer_learning/real_transfer_learning/topology.csv'\n",
    "topology_old_path = 'topology.csv'\n",
    "bestweights_old_path = 'bestweights_job.h5'\n",
    "bestweights_new_path = '/ceph01/projects/AGRamirez_misc/carpeta_alberto_moreno/datos/transfer_learning/bestweights_job.h5'\n",
    "\n",
    "# Especificar las capas a transferir (por índices, e.g., 0 para LocallyDirected_0)\n",
    "capas_a_transferir = [0]  \n",
    "\n",
    "# Cargar las topologías\n",
    "topology_nuevo = pd.read_csv(topology_nuevo_path)\n",
    "topology_old = pd.read_csv(topology_old_path)\n",
    "\n",
    "def obtener_nombres_capas(capa):\n",
    "    \"\"\"\n",
    "    Dada una capa 'n', retorna los nombres de las capas para el kernel y el bias.\n",
    "    \"\"\"\n",
    "    layer0_name = f'layer{2 * capa}_name'      # Para el kernel de LocallyDirected_n\n",
    "    layer1_name = f'layer{2 * capa + 1}_name'  # Para el bias de LocallyDirected_n\n",
    "    return layer0_name, layer1_name\n",
    "\n",
    "def obtener_topology_unique(topology, capas):\n",
    "    \"\"\"\n",
    "    Para cada capa en 'capas', elimina duplicados basados en 'layer{2n+1}_name' para asegurar mapeo único de biases.\n",
    "    \"\"\"\n",
    "    topology_unique = {}\n",
    "    for capa in capas:\n",
    "        layer1_name = f'layer{2 * capa + 1}_name'\n",
    "        if layer1_name in topology.columns:\n",
    "            topology_unique[capa] = topology.drop_duplicates(subset=[layer1_name])\n",
    "        else:\n",
    "            raise ValueError(f\"La columna {layer1_name} no existe en la topología antigua.\")\n",
    "    return topology_unique\n",
    "\n",
    "# Eliminar duplicados en 'layer{2n+1}_name' para cada capa a transferir\n",
    "topology_old_unique = obtener_topology_unique(topology_old, capas_a_transferir)\n",
    "\n",
    "# Crear mapeos de SNPs y Biases por capa\n",
    "snp_kernel_mapping = {}\n",
    "bias_mapping_old_unique = {}\n",
    "\n",
    "for capa in capas_a_transferir:\n",
    "    layer0_name, layer1_name = obtener_nombres_capas(capa)\n",
    "    \n",
    "    # Verificar que las columnas existen en la topología antigua\n",
    "    if layer0_name not in topology_old.columns or layer1_name not in topology_old.columns:\n",
    "        raise ValueError(f\"Las columnas {layer0_name} o {layer1_name} no existen en la topología antigua.\")\n",
    "    \n",
    "    # Crear mapeo SNP -> índice para el kernel de esta capa\n",
    "    snp_kernel_mapping[capa] = {snp: idx for idx, snp in enumerate(topology_old[layer0_name])}\n",
    "    \n",
    "    # Crear mapeo Bias -> índice único para el bias de esta capa\n",
    "    bias_mapping_old_unique[capa] = {bias: idx for idx, bias in enumerate(topology_old_unique[capa][layer1_name])}\n",
    "\n",
    "# Identificar SNPs y Biases coincidentes por capa\n",
    "snps_coincidentes_por_capa = {}\n",
    "bias_coincidentes_por_capa = {}\n",
    "\n",
    "for capa in capas_a_transferir:\n",
    "    layer0_name, layer1_name = obtener_nombres_capas(capa)\n",
    "    \n",
    "    snps_nuevo_numbers = set(topology_nuevo[layer0_name])\n",
    "    bias_nuevo_names = set(topology_nuevo[layer1_name])\n",
    "    \n",
    "    snps_old_numbers = set(topology_old[layer0_name])\n",
    "    bias_old_names = set(topology_old_unique[capa][layer1_name])\n",
    "    \n",
    "    snps_coincidentes = snps_nuevo_numbers.intersection(snps_old_numbers)\n",
    "    bias_coincidentes = bias_nuevo_names.intersection(bias_old_names)\n",
    "    \n",
    "    snps_coincidentes_por_capa[capa] = snps_coincidentes\n",
    "    bias_coincidentes_por_capa[capa] = bias_coincidentes\n",
    "\n",
    "# Cargar los pesos originales\n",
    "with h5py.File(bestweights_old_path, 'r') as f:\n",
    "    # Cargar kernels y biases de las capas LocallyDirected especificadas\n",
    "    kernels_old = {}\n",
    "    biases_old = {}\n",
    "    for capa in capas_a_transferir:\n",
    "        layer_name = f'LocallyDirected_{capa}'\n",
    "        try:\n",
    "            kernels_old[capa] = f[f'model_weights/{layer_name}/{layer_name}/kernel:0'][:]\n",
    "            biases_old[capa] = f[f'model_weights/{layer_name}/{layer_name}/bias:0'][:]\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"No se encontró la capa {layer_name} en el archivo de pesos antiguos: {e}\")\n",
    "    \n",
    "    # Cargar parámetros de Batch Normalization correspondientes\n",
    "    moving_mean_old = {}\n",
    "    moving_variance_old = {}\n",
    "    for capa in capas_a_transferir:\n",
    "        bn_layer = f'batch_normalization_{capa}' if capa > 0 else 'batch_normalization'\n",
    "        try:\n",
    "            moving_mean_old[capa] = f[f'model_weights/{bn_layer}/{bn_layer}/moving_mean:0'][:]\n",
    "            moving_variance_old[capa] = f[f'model_weights/{bn_layer}/{bn_layer}/moving_variance:0'][:]\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"No se encontró la capa de batch normalization {bn_layer} en el archivo de pesos antiguos: {e}\")\n",
    "    \n",
    "    # Cargar pesos de la capa de salida (Dense), NO TIENE QUE SER NECESARIO, EN NUESTRO CASO SI PQ EL TRANSFER ERA HASTA LA ULTIMA CAPA\n",
    "    try:\n",
    "        output_kernel_old = f['model_weights/output_layer/output_layer/kernel:0'][:]\n",
    "        output_bias_old = f['model_weights/output_layer/output_layer/bias:0'][:]  # Copia directa del bias de salida\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"No se encontró la capa de salida 'output_layer' en el archivo de pesos antiguos: {e}\")\n",
    "\n",
    "# Inicializar estructuras para los nuevos pesos\n",
    "kernel_nuevo = {}\n",
    "bias_nuevo = {}\n",
    "moving_mean_nuevo = {}\n",
    "moving_variance_nuevo = {}\n",
    "\n",
    "# Transferencia de pesos por capa\n",
    "for capa in capas_a_transferir:\n",
    "    layer0_name, layer1_name = obtener_nombres_capas(capa)\n",
    "    \n",
    "    # Obtener SNPs y Biases coincidentes para esta capa\n",
    "    snps_coincidentes = snps_coincidentes_por_capa[capa]\n",
    "    bias_coincidentes = bias_coincidentes_por_capa[capa]\n",
    "    \n",
    "    # Obtener mapeos específicos para esta capa\n",
    "    snp_mapping = snp_kernel_mapping[capa]\n",
    "    bias_mapping = bias_mapping_old_unique[capa]\n",
    "    \n",
    "    # Obtener índices de SNPs coincidentes\n",
    "    indices_coincidentes_snps = [snp_mapping[snp] for snp in snps_coincidentes if snp in snp_mapping]\n",
    "    \n",
    "    # Extraer kernel y bias antiguos\n",
    "    kernel_antiguo = kernels_old[capa]\n",
    "    bias_antiguo = biases_old[capa]\n",
    "    \n",
    "    # Crear nuevo kernel solo con SNPs coincidentes\n",
    "    kernel_nuevo[capa] = kernel_antiguo[indices_coincidentes_snps]\n",
    "    \n",
    "    # Inicializar nuevo bias con copia del bias antiguo\n",
    "    bias_nuevo[capa] = np.copy(bias_antiguo)\n",
    "    \n",
    "    # Identificar SNPs faltantes en la nueva topología\n",
    "    snps_faltantes = set(topology_old[layer0_name]).difference(set(topology_nuevo[layer0_name]))\n",
    "    print(f\"Capas {capa} - Número de SNPs faltantes calculados: {len(snps_faltantes)}\")\n",
    "    \n",
    "    # Ajustar biases para SNPs faltantes\n",
    "    for layer1, group in topology_old[topology_old[layer0_name].isin(snps_faltantes)].groupby(layer1_name):\n",
    "        snps_grupo = set(group[layer0_name])\n",
    "        # Sumar los kernels correspondientes a los SNPs faltantes\n",
    "        total_peso_faltante = np.sum([kernel_antiguo[snp_mapping[snp]] for snp in snps_grupo if snp in snp_mapping], axis=0)\n",
    "        \n",
    "        if layer1 in bias_mapping:\n",
    "            idx_bias = bias_mapping[layer1]\n",
    "            if idx_bias < len(bias_nuevo[capa]):\n",
    "                bias_nuevo[capa][idx_bias] += total_peso_faltante\n",
    "            else:\n",
    "                print(f\"Capas {capa} - Índice fuera de rango: idx_bias={idx_bias}, no se puede ajustar.\")\n",
    "        else:\n",
    "            print(f\"Capas {capa} - Layer1_name '{layer1}' no está en bias_mapping_old_unique\")\n",
    "    \n",
    "    # Recalcular parámetros de Batch Normalization usando el nuevo kernel\n",
    "    moving_mean_nuevo[capa] = np.mean(kernel_nuevo[capa], axis=0)\n",
    "    moving_variance_nuevo[capa] = np.var(kernel_nuevo[capa], axis=0)\n",
    "\n",
    "# Ajustar la capa output_layer (Dense) solo si es necesario\n",
    "# En este caso, no es necesario ajustar la capa de salida\n",
    "output_kernel_nuevo = output_kernel_old.copy()  # Mantener el kernel de salida original\n",
    "output_bias_nuevo = np.copy(output_bias_old)    # Copia directa del bias de salida sin ajustes\n",
    "\n",
    "# Guardar los nuevos pesos en un archivo HDF5\n",
    "with h5py.File(bestweights_new_path, 'w') as f:\n",
    "    # Guardar los nuevos pesos de cada capa LocallyDirected especificada\n",
    "    for capa in capas_a_transferir:\n",
    "        layer_name = f'LocallyDirected_{capa}'\n",
    "        f.create_dataset(f'model_weights/{layer_name}/{layer_name}/kernel:0', data=kernel_nuevo[capa])\n",
    "        f.create_dataset(f'model_weights/{layer_name}/{layer_name}/bias:0', data=bias_nuevo[capa])\n",
    "        \n",
    "        # Guardar los nuevos pesos de batch normalization\n",
    "        bn_layer = f'batch_normalization_{capa}' if capa > 0 else 'batch_normalization'\n",
    "        f.create_dataset(f'model_weights/{bn_layer}/{bn_layer}/moving_mean:0', data=moving_mean_nuevo[capa])\n",
    "        f.create_dataset(f'model_weights/{bn_layer}/{bn_layer}/moving_variance:0', data=moving_variance_nuevo[capa])\n",
    "        \n",
    "        # Crear la capa de activación vacía correspondiente a esta capa LocallyDirected\n",
    "        activation_layer = 'activation' if capa == 0 else f'activation_{capa}'\n",
    "        f.create_dataset(f'model_weights/{activation_layer}/{activation_layer}', data=[])\n",
    "    \n",
    "    # Guardar los nuevos pesos de la capa Dense (output_layer) sin modificaciones, NO TIENE PORQUÉ SER NECESARIO\n",
    "    f.create_dataset('model_weights/output_layer/output_layer/kernel:0', data=output_kernel_nuevo)\n",
    "    f.create_dataset('model_weights/output_layer/output_layer/bias:0', data=output_bias_nuevo)  # Copia directa del bias de salida\n",
    "    \n",
    "    # Mantener la estructura de activaciones vacías para capas no transferidas (si es necesario)\n",
    "    # Esto asegura que todas las capas de activación existentes en la topología antigua estén presentes\n",
    "    activaciones_existentes = [col for col in topology_old.columns if col.startswith('activation')]\n",
    "    for activation in activaciones_existentes:\n",
    "        activation_path = f'model_weights/{activation}/{activation}'\n",
    "        if activation not in f['model_weights']:\n",
    "            f.create_dataset(activation_path, data=[])\n",
    "    \n",
    "    print(f\"Nuevos archivos de pesos guardados en: {bestweights_new_path}\")\n",
    "\n",
    "# Función para imprimir la estructura de capas en un archivo HDF5\n",
    "def imprimir_estructura_hdf5(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Imprime la estructura de capas y datasets en el archivo HDF5 especificado.\n",
    "    \"\"\"\n",
    "    with h5py.File(ruta_archivo, 'r') as f:\n",
    "        print(f\"\\nEstructura de capas en '{ruta_archivo}':\")\n",
    "        def imprimir_grupo(grupo, indent=0):\n",
    "            for key in grupo:\n",
    "                item = grupo[key]\n",
    "                print(\"    \" * indent + f\"- {key}\")\n",
    "                if isinstance(item, h5py.Group):\n",
    "                    imprimir_grupo(item, indent + 1)\n",
    "                elif isinstance(item, h5py.Dataset):\n",
    "                    print(\"    \" * (indent + 1) + f\"Dataset: {key}, Shape: {item.shape}, Dtype: {item.dtype}\")\n",
    "        \n",
    "        imprimir_grupo(f)\n",
    "\n",
    "# Imprimir la estructura del nuevo archivo HDF5\n",
    "imprimir_estructura_hdf5(bestweights_new_path)\n",
    "\n",
    "print(\"Proceso de transfer learning completado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer especifico que apliqué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Rutas a los archivos\n",
    "topology_nuevo_path = '/ceph01/projects/AGRamirez_misc/carpeta_alberto_moreno/datos/transfer_learning/real_transfer_learning/topology.csv'\n",
    "topology_old_path = '/ceph01/projects/AGRamirez_misc/campos/Alberto/tot_Federico/data_Frederico/hg38_genes/topology.csv'\n",
    "bestweights_old_path = '/ceph01/projects/AGRamirez_misc/campos/Alberto/tot_Federico/data_Frederico/bestweights_job.h5'\n",
    "bestweights_new_path = '/ceph01/projects/AGRamirez_misc/carpeta_alberto_moreno/datos/transfer_learning/real_transfer_learning/bestweights_job.h5'\n",
    "\n",
    "# Cargar las topologías\n",
    "topology_nuevo = pd.read_csv(topology_nuevo_path)\n",
    "topology_old = pd.read_csv(topology_old_path)\n",
    "\n",
    "# Eliminar duplicados en `layer1_name` para asegurar mapeo único\n",
    "topology_old_unique = topology_old.drop_duplicates(subset=['layer1_name'])\n",
    "\n",
    "# Crear mapeo entre los SNPs y los bias en `topology_old_unique`\n",
    "snp_kernel_mapping = {snp: idx for idx, snp in enumerate(topology_old['layer0_name'])}\n",
    "bias_mapping_old_unique = {bias: idx for idx, bias in enumerate(topology_old_unique['layer1_name'])}\n",
    "\n",
    "# Identificar SNPs y biases coincidentes entre la nueva topología y la vieja topología\n",
    "snps_nuevo_numbers = set(topology_nuevo['layer0_name'])\n",
    "bias_nuevo_names = set(topology_nuevo['layer1_name'])\n",
    "snps_old_numbers = set(topology_old['layer0_name'])\n",
    "bias_old_names = set(topology_old_unique['layer1_name'])\n",
    "snps_coincidentes = snps_nuevo_numbers.intersection(snps_old_numbers)\n",
    "bias_coincidentes = bias_nuevo_names.intersection(bias_old_names)\n",
    "\n",
    "# Cargar los pesos originales\n",
    "with h5py.File(bestweights_old_path, 'r') as f:\n",
    "    # LocallyDirected_0 pesos\n",
    "    kernel_old = f['model_weights/LocallyDirected_0/LocallyDirected_0/kernel:0'][:]\n",
    "    bias_old = f['model_weights/LocallyDirected_0/LocallyDirected_0/bias:0'][:]\n",
    "    \n",
    "    # Batch normalization pesos\n",
    "    moving_mean_old = f['model_weights/batch_normalization/batch_normalization/moving_mean:0'][:]\n",
    "    moving_variance_old = f['model_weights/batch_normalization/batch_normalization/moving_variance:0'][:]\n",
    "    \n",
    "    # Output layer (Dense) pesos\n",
    "    output_kernel_old = f['model_weights/output_layer/output_layer/kernel:0'][:]\n",
    "    output_bias_old = f['model_weights/output_layer/output_layer/bias:0'][:]  # Copia directa del bias de salida\n",
    "\n",
    "# Crear un nuevo kernel solo con los SNPs coincidentes en LocallyDirected_0\n",
    "indices_coincidentes_snps = [snp_kernel_mapping[snp] for snp in snps_coincidentes]\n",
    "kernel_nuevo = kernel_old[indices_coincidentes_snps]\n",
    "\n",
    "# Inicializar bias_nuevo con una copia de bias_old para LocallyDirected_0\n",
    "bias_nuevo = np.copy(bias_old)\n",
    "\n",
    "# Identificar los SNPs faltantes en la nueva topología que no tienen coincidencias en la vieja topología\n",
    "snps_faltantes = snps_old_numbers.difference(snps_nuevo_numbers)\n",
    "print(\"Número de SNPs faltantes calculados:\", len(snps_faltantes))\n",
    "\n",
    "# Ajustar los bias de LocallyDirected_0 para los SNPs faltantes\n",
    "for layer1_name, group in topology_old[topology_old['layer0_name'].isin(snps_faltantes)].groupby('layer1_name'):\n",
    "    snps_grupo = set(group['layer0_name'])\n",
    "    total_peso_faltante = np.sum([kernel_old[snp_kernel_mapping[snp]] for snp in snps_grupo])\n",
    "\n",
    "    if layer1_name in bias_mapping_old_unique:\n",
    "        idx_bias = bias_mapping_old_unique[layer1_name]\n",
    "        if idx_bias < len(bias_nuevo):\n",
    "            bias_nuevo[idx_bias] += total_peso_faltante\n",
    "        else:\n",
    "            print(f\"Índice fuera de rango: idx_bias={idx_bias}, no se puede ajustar.\")\n",
    "    else:\n",
    "        print(f\"Layer1_name {layer1_name} no está en bias_mapping_old_unique\")\n",
    "\n",
    "# Ajustar la capa output_layer (Dense)\n",
    "output_kernel_nuevo = output_kernel_old[:len(indices_coincidentes_snps), :]  # Ajuste del kernel para los SNPs coincidentes\n",
    "output_bias_nuevo = np.copy(output_bias_old)  # Copia directa del bias de salida sin ajustes\n",
    "\n",
    "# Aquí usamos el kernel_nuevo para recalcular los parámetros de batch normalization\n",
    "moving_mean_nuevo = np.mean(kernel_nuevo, axis=0)\n",
    "moving_variance_nuevo = np.var(kernel_nuevo, axis=0)\n",
    "\n",
    "# Guardar los nuevos pesos en un archivo HDF5\n",
    "with h5py.File(bestweights_new_path, 'w') as f:\n",
    "    # Guardar los nuevos pesos del kernel y bias de LocallyDirected_0\n",
    "    f.create_dataset('model_weights/LocallyDirected_0/LocallyDirected_0/kernel:0', data=kernel_nuevo)\n",
    "    f.create_dataset('model_weights/LocallyDirected_0/LocallyDirected_0/bias:0', data=bias_nuevo)\n",
    "\n",
    "    # Guardar los nuevos pesos de batch normalization (solo los relacionados con LocallyDirected_0)\n",
    "    f.create_dataset('model_weights/batch_normalization/batch_normalization/moving_mean:0', data=moving_mean_nuevo)\n",
    "    f.create_dataset('model_weights/batch_normalization/batch_normalization/moving_variance:0', data=moving_variance_nuevo)\n",
    "\n",
    "    # Guardar los nuevos pesos del kernel y bias de la capa Dense (output_layer)\n",
    "    f.create_dataset('model_weights/output_layer/output_layer/kernel:0', data=output_kernel_nuevo)\n",
    "    f.create_dataset('model_weights/output_layer/output_layer/bias:0', data=output_bias_nuevo)  # Copia directa del bias de salida\n",
    "\n",
    "    # Crear la capa de activación vacía (sin parámetros), puede que no sea necesario pero de esta manera nos aseguramos la estructura que queremos\n",
    "    f.create_dataset('model_weights/activation/activation', data=[])\n",
    "    f.create_dataset('model_weights/activation_1/activation_1', data=[])\n",
    "\n",
    "print(\"Nuevo archivo de pesos guardado en:\", bestweights_new_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
